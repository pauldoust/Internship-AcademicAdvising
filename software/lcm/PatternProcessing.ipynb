{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "33\n",
      "ss4\n",
      "4\n",
      "cell:  2871     8.00\n",
      "5568     7.35\n",
      "6364     6.85\n",
      "6778     6.00\n",
      "6939     7.15\n",
      "7360     8.15\n",
      "7617     6.30\n",
      "8246     7.45\n",
      "8465     8.10\n",
      "8894     6.35\n",
      "9258     6.05\n",
      "9598     9.65\n",
      "10070    6.05\n",
      "10429    6.55\n",
      "11090    9.05\n",
      "11215    8.30\n",
      "11425    7.00\n",
      "12816    7.25\n",
      "13154    8.25\n",
      "13855    7.00\n",
      "13952    6.35\n",
      "14110    7.10\n",
      "14277    8.65\n",
      "14415    6.70\n",
      "14565    7.10\n",
      "14709    8.00\n",
      "14778    8.00\n",
      "15026    8.85\n",
      "15162    6.95\n",
      "15216    7.90\n",
      "15349    6.70\n",
      "15394    8.15\n",
      "15898    8.80\n",
      "17790    9.20\n",
      "17905    7.95\n",
      "19249    8.20\n",
      "19913    8.00\n",
      "Name: GRADE, dtype: float64\n",
      "ss5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "\n",
    "\n",
    "\n",
    "Experiment = \"\\SecondSemester\"\n",
    "DataFolder = \"\\data\\\\\"\n",
    "Path = r\"C:\\Users\\mpouldou\\Desktop\\lcm\" + Experiment + DataFolder\n",
    "Features_Set = set([ 'NEW_COD_STUDENT', 'SEMESTER', 'YEAR', 'COURSE_ID', 'GRADE', 'GPA', 'PROFESSOR_ID', 'ID', 'Course_encode', 'COURSE_NAME'])\n",
    "\n",
    "\n",
    "# def concatentated_grades(x):\n",
    "#      return \",\".join([str(y) for y in x])\n",
    "\n",
    "def concatentated_courses(x):\n",
    "    if len(x.values) <= 2:\n",
    "        return \"\"\n",
    "    return \" \".join([str(y) for y in x])\n",
    "#     return \" \".join([str(y) for y in x])\n",
    "# \treturn \"\"\n",
    "\n",
    "def join_historic_data( historia_academia_path, teaching_materials_path, graduates_all_dep_path):\n",
    "    df_historia_academia = pd.read_csv(historia_academia_path)\n",
    "    df_teaching_materials = pd.read_csv(teaching_materials_path)\n",
    "    df_graduates_all_dep = pd.read_csv(graduates_all_dep_path)\n",
    "    print(\"1\")\n",
    "    \n",
    "    df_historia_academia.GRADE = df_historia_academia.GRADE[df_historia_academia.GRADE != \"0,00\"]\n",
    "    df_historia_academia = pd.merge(df_graduates_all_dep, df_historia_academia, on=[\"NEW_COD_STUDENT\"])\n",
    "\n",
    "    print(\"2\")\n",
    "    df_historia_academia_extended = pd.merge(df_historia_academia, df_teaching_materials, on=[\"COURSE_ID\"])\n",
    "    # df_historia_academia_extended = pd.merge(df_historia_academia, df_teaching_materials, on=[\"COURSE_ID\", \"COURSE_TRACK\", \"YEAR\", \"SEMESTER\"])\n",
    "\n",
    "    df_historia_academia_extended.SEMESTER = df_historia_academia_extended.SEMESTER.apply(lambda x: \"1T\" if x == \"1S\" else x)\n",
    "    df_historia_academia_extended.SEMESTER = df_historia_academia_extended.SEMESTER.apply(lambda x: \"2T\" if x == \"2S\" else x)\n",
    "    df_historia_academia_extended.SEMESTER = df_historia_academia_extended.SEMESTER.apply(lambda x: \"3T\" if x == \"3S\" else x)\n",
    "    print(\"3\")\n",
    "\n",
    "    df_historia_academia_extended2 = df_historia_academia_extended.to_csv (Path + \"historia_academia_extended.csv\", index = None, header=True) \n",
    "    print(\"33\")\n",
    "\n",
    "#     df_dataset = pd.merge(df_graduates_all_dep, df_historia_academia_extended, on=[\"NEW_COD_STUDENT\"])\n",
    "\n",
    "    \n",
    "#     df_dataset2 = df_dataset.to_csv (Path + \"dataset_joined.csv\", index = None, header=True) \n",
    "\n",
    "    return df_historia_academia_extended\n",
    "\n",
    "\n",
    "def clean_dataset( joined_data):\n",
    "    print(\"4\")\n",
    "\n",
    "    joined_data.SEMESTER = joined_data.SEMESTER.apply(lambda x: \"1T\" if x == \"1S\" else x)\n",
    "    joined_data.SEMESTER = joined_data.SEMESTER.apply(lambda x: \"2T\" if x == \"2S\" else x)\n",
    "    joined_data.SEMESTER = joined_data.SEMESTER.apply(lambda x: \"3T\" if x == \"3S\" else x)\n",
    "\n",
    "    joined_data.SEMESTER = joined_data.SEMESTER.apply(lambda x: \"1T\" if x == \"1B\" else x)\n",
    "    joined_data.SEMESTER = joined_data.SEMESTER.apply(lambda x: \"2T\" if x == \"2B\" else x)\n",
    "    joined_data.SEMESTER = joined_data.SEMESTER.apply(lambda x: \"3T\" if x == \"3B\" else x)\n",
    "\n",
    "\n",
    "    joined_data.SEMESTER = joined_data.SEMESTER.apply(lambda x: \"4T\" if x == \"4B\" else x)\n",
    "    joined_data.SEMESTER = joined_data.SEMESTER.apply(lambda x: \"5T\" if x == \"5B\" else x)\n",
    "    joined_data.SEMESTER = joined_data.SEMESTER.apply(lambda x: \"6T\" if x == \"6B\" else x)\n",
    "    joined_data.SEMESTER = joined_data.SEMESTER.apply(lambda x: \"7T\" if x == \"7B\" else x)\n",
    "\n",
    "    joined_data.GRADE = joined_data.GRADE.str.replace(\",\", \".\", case = False)\n",
    "    # print(joined_data.GRADE)\n",
    "    joined_data.GRADE = pd.to_numeric(joined_data.GRADE)\n",
    "    joined_data.GRADE = joined_data.GRADE[joined_data.GRADE != 0]\n",
    "    joined_data.GRADE = joined_data.GRADE[joined_data.GRADE != None]\n",
    "    joined_data.GRADE = joined_data.GRADE[joined_data.GRADE.notna()]\n",
    "    joined_data = joined_data[joined_data.GRADE.notnull()]\n",
    "    joined_data.GRADE = joined_data.GRADE[joined_data.GRADE != np.nan]\n",
    "    \n",
    "#     print('cell: ', joined_data[joined_data.NEW_COD_STUDENT == 1451].GRADE)\n",
    "    \n",
    "    for column in joined_data:\n",
    "    \tif column not in Features_Set:\n",
    "    \t\tjoined_data = joined_data.drop(column, axis=1)\n",
    "\n",
    "    df_flow2 = joined_data.to_csv (Path + Experiment + \"_flow.csv\" , index = None, header=True)\n",
    "    return joined_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convertToTransactions(cleanded_data):\n",
    "\ttransactions = cleanded_data.groupby(['NEW_COD_STUDENT', 'YEAR', 'SEMESTER']).agg({'Course_encode': concatentated_courses, 'COURSE_NAME':concatentated_courses, 'COURSE_ID' : concatentated_courses}).reset_index()\n",
    "\ttransactions = transactions[transactions.Course_encode !=\"\"]\n",
    "\ttransactions2 = transactions.to_csv(Path + Experiment + \"_transaction.csv\" , index = None, header=True)\n",
    "\treturn transactions\n",
    "\n",
    "def main():\n",
    "    historia_academia_path = Path + \"historia_academica_anonym.csv\"\n",
    "    teaching_materials_path = Path + \"materias_anonym_clean_filtered.csv\"\n",
    "    graduates_all_dep = Path + \"graduadosComputacion_anonym.csv\"\n",
    "    df_dataset = join_historic_data(historia_academia_path, teaching_materials_path, graduates_all_dep)\n",
    "    print(\"ss4\")\n",
    "    cleaned_data = clean_dataset(df_dataset)\n",
    "    transaction_table = convertToTransactions(cleaned_data)\n",
    "    # print(transaction_table)\n",
    "    transaction_table  = transaction_table[transaction_table.Course_encode !=\"\"]\n",
    "    np.savetxt(r'C:\\Users\\mpouldou\\Desktop\\lcm\\transactions.txt', transaction_table.Course_encode, delimiter=\" \", fmt=\"%s\")\n",
    "    print(\"ss5\")\n",
    "\n",
    "\n",
    "    \n",
    "main()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currentSet:  {18, 1639, 807}\n",
      "processedSets:  []\n",
      "finally:  [{18, 1639, 807}]\n",
      "*********\n",
      "currentSet:  {416, 65, 428}\n",
      "processedSets:  [{18, 1639, 807}]\n",
      "finally:  [{18, 1639, 807}, {416, 65, 428}]\n",
      "*********\n",
      "currentSet:  {137, 148, 193}\n",
      "processedSets:  [{18, 1639, 807}, {416, 65, 428}]\n",
      "finally:  [{18, 1639, 807}, {416, 65, 428}, {137, 148, 193}]\n",
      "*********\n",
      "currentSet:  {137, 426, 148}\n",
      "processedSets:  [{18, 1639, 807}, {416, 65, 428}, {137, 148, 193}]\n",
      "finally:  [{18, 1639, 807}, {416, 65, 428}, {137, 148, 193}, {137, 426, 148}]\n",
      "*********\n",
      "currentSet:  {137, 426, 148, 722}\n",
      "processedSets:  [{18, 1639, 807}, {416, 65, 428}, {137, 148, 193}, {137, 426, 148}]\n",
      "remove:  {137, 426, 148} is contained in:  {137, 426, 148, 722}\n",
      "finally:  [{18, 1639, 807}, {416, 65, 428}, {137, 148, 193}, {137, 426, 148}, {137, 426, 148, 722}]\n",
      "*********\n",
      "currentSet:  {137, 722, 148}\n",
      "processedSets:  [{18, 1639, 807}, {416, 65, 428}, {137, 148, 193}, {137, 426, 148}, {137, 426, 148, 722}]\n",
      "{137, 722, 148} is contained in:  {137, 426, 148, 722}\n",
      "*********\n",
      "currentSet:  {137, 426, 722}\n",
      "processedSets:  [{18, 1639, 807}, {416, 65, 428}, {137, 148, 193}, {137, 426, 148}, {137, 426, 148, 722}]\n",
      "{137, 426, 722} is contained in:  {137, 426, 148, 722}\n",
      "*********\n",
      "currentSet:  {168, 138, 428}\n",
      "processedSets:  [{18, 1639, 807}, {416, 65, 428}, {137, 148, 193}, {137, 426, 148}, {137, 426, 148, 722}]\n",
      "finally:  [{18, 1639, 807}, {416, 65, 428}, {137, 148, 193}, {137, 426, 148}, {137, 426, 148, 722}, {168, 138, 428}]\n",
      "*********\n",
      "currentSet:  {416, 138, 428}\n",
      "processedSets:  [{18, 1639, 807}, {416, 65, 428}, {137, 148, 193}, {137, 426, 148}, {137, 426, 148, 722}, {168, 138, 428}]\n",
      "finally:  [{18, 1639, 807}, {416, 65, 428}, {137, 148, 193}, {137, 426, 148}, {137, 426, 148, 722}, {168, 138, 428}, {416, 138, 428}]\n",
      "*********\n",
      "currentSet:  {416, 138, 428, 788}\n",
      "processedSets:  [{18, 1639, 807}, {416, 65, 428}, {137, 148, 193}, {137, 426, 148}, {137, 426, 148, 722}, {168, 138, 428}, {416, 138, 428}]\n",
      "remove:  {416, 138, 428} is contained in:  {416, 138, 428, 788}\n",
      "finally:  [{18, 1639, 807}, {416, 65, 428}, {137, 148, 193}, {137, 426, 148}, {137, 426, 148, 722}, {168, 138, 428}, {416, 138, 428}, {416, 138, 428, 788}]\n",
      "*********\n",
      "currentSet:  {416, 138, 788}\n",
      "processedSets:  [{18, 1639, 807}, {416, 65, 428}, {137, 148, 193}, {137, 426, 148}, {137, 426, 148, 722}, {168, 138, 428}, {416, 138, 428}, {416, 138, 428, 788}]\n",
      "{416, 138, 788} is contained in:  {416, 138, 428, 788}\n",
      "*********\n",
      "currentSet:  {722, 138, 428}\n",
      "processedSets:  [{18, 1639, 807}, {416, 65, 428}, {137, 148, 193}, {137, 426, 148}, {137, 426, 148, 722}, {168, 138, 428}, {416, 138, 428}, {416, 138, 428, 788}]\n",
      "finally:  [{18, 1639, 807}, {416, 65, 428}, {137, 148, 193}, {137, 426, 148}, {137, 426, 148, 722}, {168, 138, 428}, {416, 138, 428}, {416, 138, 428, 788}, {722, 138, 428}]\n",
      "*********\n",
      "currentSet:  {722, 426, 148}\n",
      "processedSets:  [{18, 1639, 807}, {416, 65, 428}, {137, 148, 193}, {137, 426, 148}, {137, 426, 148, 722}, {168, 138, 428}, {416, 138, 428}, {416, 138, 428, 788}, {722, 138, 428}]\n",
      "{722, 426, 148} is contained in:  {137, 426, 148, 722}\n",
      "*********\n",
      "currentSet:  {416, 722, 428}\n",
      "processedSets:  [{18, 1639, 807}, {416, 65, 428}, {137, 148, 193}, {137, 426, 148}, {137, 426, 148, 722}, {168, 138, 428}, {416, 138, 428}, {416, 138, 428, 788}, {722, 138, 428}]\n",
      "finally:  [{18, 1639, 807}, {416, 65, 428}, {137, 148, 193}, {137, 426, 148}, {137, 426, 148, 722}, {168, 138, 428}, {416, 138, 428}, {416, 138, 428, 788}, {722, 138, 428}, {416, 722, 428}]\n",
      "*********\n",
      "currentSet:  {416, 428, 788}\n",
      "processedSets:  [{18, 1639, 807}, {416, 65, 428}, {137, 148, 193}, {137, 426, 148}, {137, 426, 148, 722}, {168, 138, 428}, {416, 138, 428}, {416, 138, 428, 788}, {722, 138, 428}, {416, 722, 428}]\n",
      "{416, 428, 788} is contained in:  {416, 138, 428, 788}\n",
      "*********\n",
      "final Sets:  [{18, 1639, 807}, {416, 65, 428}, {137, 148, 193}, {137, 426, 148}, {137, 426, 148, 722}, {168, 138, 428}, {416, 138, 428}, {416, 138, 428, 788}, {722, 138, 428}, {416, 722, 428}]\n",
      "writing:  {18, 1639, 807}\n",
      "**************\n",
      "writing:  {416, 65, 428}\n",
      "**************\n",
      "writing:  {137, 148, 193}\n",
      "**************\n",
      "writing:  {137, 426, 148}\n",
      "**************\n",
      "writing:  {137, 426, 148, 722}\n",
      "**************\n",
      "writing:  {168, 138, 428}\n",
      "**************\n",
      "writing:  {416, 138, 428}\n",
      "**************\n",
      "writing:  {416, 138, 428, 788}\n",
      "**************\n",
      "writing:  {722, 138, 428}\n",
      "**************\n",
      "writing:  {416, 722, 428}\n",
      "**************\n"
     ]
    }
   ],
   "source": [
    "def ProcessedBefore (processedSets, newSet):\n",
    "    print('currentSet: ', newSet)\n",
    "    print('processedSets: ', str(processedSets))\n",
    "    replace = False\n",
    "    for processedSet in processedSets:\n",
    "        if processedSet == newSet:\n",
    "            return True\n",
    "        if newSet.issubset(processedSet):\n",
    "            print(str(newSet), \"is contained in: \", str(processedSet))\n",
    "            return True\n",
    "        elif processedSet.issubset(newSet):\n",
    "            print('remove: ', str(processedSet), \"is contained in: \", str(newSet))\n",
    "            processedSet = 'None'\n",
    "            replace = True\n",
    "    \n",
    "    processedSets.append(newSet)\n",
    "#     if len(processedSets) == 0 or replace == True:\n",
    "#         processedSets.append(newSet)\n",
    "\n",
    "    print(\"finally: \", str(processedSets))\n",
    "\n",
    "Experiment = \"\\SecondSemester\"\n",
    "DataFolder = \"\\data\\\\\"\n",
    "Path = r\"C:\\Users\\mpouldou\\Desktop\\lcm\" + Experiment + DataFolder\n",
    "\n",
    "f = open(r'C:\\Users\\mpouldou\\Desktop\\lcm\\out.txt', \"r\")\n",
    "fw = open(r'C:\\Users\\mpouldou\\Desktop\\lcm\\out_filtered.txt',\"w+\")\n",
    "teaching_materials_path = Path + \"materias_anonym_clean_filtered.csv\"\n",
    "df_teaching_materials = pd.read_csv(teaching_materials_path)\n",
    "processedSets = []\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    transaction = line.split(\"#SUP:\")[0]\n",
    "    if len(transaction.split()) < 3:\n",
    "        pass\n",
    "    else:\n",
    "        newLine = \"\"\n",
    "        currentSet = set()\n",
    "        for course in transaction.split():\n",
    "            courseCode = df_teaching_materials.loc[df_teaching_materials['Course_encode'] == int(course)].iloc[0].COURSE_ID\n",
    "#             print(courseCode.COURSE_ID)\n",
    "#             newLine = newLine + str(course) + \" \"\n",
    "            currentSet.add(int(course))\n",
    "        ProcessedBefore(processedSets,  currentSet)\n",
    "        print('*********')\n",
    "\n",
    "        \n",
    "    \n",
    "    # check if line is not empty\n",
    "    if not line:\n",
    "        break\n",
    "print(\"final Sets: \", processedSets)\n",
    "for finalSet in processedSets:\n",
    "    print(\"writing: \", str(finalSet))\n",
    "    print(\"**************\")\n",
    "    newLine = \"\"\n",
    "    for item in finalSet:\n",
    "        courseCode = df_teaching_materials.loc[df_teaching_materials['Course_encode'] == int(item)].iloc[0].COURSE_ID\n",
    "        newLine = newLine + str(courseCode) + \" \"\n",
    "    fw.write(str(newLine)+\"\\r\\n\")\n",
    "f.close()\n",
    "fw.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
